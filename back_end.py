# -*- coding: utf-8 -*-
"""back_end.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DdsQt_Cf94E-SibN52sW9ct2R40Tyha-
"""

import sys
from collections import Counter, OrderedDict
import itertools
from itertools import islice, count, groupby
import pandas as pd
import os
import re
from operator import itemgetter
import nltk
from nltk.stem.porter import *
from nltk.corpus import stopwords
from time import time
from timeit import timeit
from pathlib import Path
import pickle
import pandas as pd
import numpy as np
from google.cloud import storage
global index_object
global merged_dict
import pickle

client = storage.Client()
bucket_name = 'bucket_324879501'
directory_name = 'title/'

index_object = None

blobs = client.list_blobs(bucket_name, prefix=directory_name)

for blob in blobs:
    if blob.name.endswith('index.pkl'):
        with open("local_index.pkl", "wb") as file_obj:
            blob.download_to_file(file_obj)

        import pickle

        with open("local_index.pkl", "rb") as file_obj:
            index_object = pickle.load(file_obj)

        break

# Ensure the index object was loaded
if index_object is not None:
    print(index_object.term_total)
else:
    print("Index file not found or failed to load.")

bucket = client.bucket(bucket_name)


def load_pickle_from_gcs(bucket_name, source_blob_name):
    """Loads a pickle file from a Google Cloud Storage bucket into a Python dictionary."""
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(source_blob_name)

    pickle_bytes = blob.download_as_bytes()
    pickle_data = io.BytesIO(pickle_bytes)

    data = pickle.load(pickle_data)
    return data

import math
from collections import defaultdict
from google.cloud import storage
import pickle
import io

source_blob_name = 'PageRank_dict.pkl'
loaded_dict = load_pickle_from_gcs(bucket_name, source_blob_name)
doc_id_to_title_dict = load_pickle_from_gcs(bucket_name, 'title_id_dict/dict_id_to_title.pkl')

PageRank_Dict = dict(loaded_dict)
def bm25_score(index, query, k1=1.5, b=0.75, k3=1.5):
    """
    Calculate BM25 score for a given query using an InvertedIndex instance.

    Parameters:
    - index: An instance of the InvertedIndex class.
    - query: A string representing the search query.
    - k1, b: BM25 parameters.

    Returns:
    A dictionary of document IDs and their BM25 scores.
    """
    score = defaultdict(float)
    query_terms = Counter(query)

    for term, freq in query_terms.items():
        if term in index.df:
            for doc_id, bm25 in index.read_a_posting_list('', term, bucket_name):

                score[doc_id] += bm25 * freq / (freq + k3)

    return score

# Sort the scores in descending order to get documents with the highest scores first
from collections import Counter, defaultdict

def combine_bm25_pagerank(bm25_scores, pagerank_dict, bm25_weight=0.5, pagerank_weight=0.5):
    """
    Combine BM25 scores with PageRank scores using a weighted average.

    Parameters:
    - bm25_scores: A dictionary of document IDs and their BM25 scores.
    - pagerank_dict: A dictionary of document IDs and their PageRank scores.
    - bm25_weight: The weight for BM25 scores in the combined score.
    - pagerank_weight: The weight for PageRank scores in the combined score.

    Returns:
    A dictionary of document IDs and their combined BM25 and PageRank scores.
    """
    combined_scores = {}

    max_bm25 = max(bm25_scores.values())
    max_pr = max(pagerank_dict.values())

    for doc_id in bm25_scores:
        normalized_bm25 = (bm25_scores[doc_id] / max_bm25) if max_bm25 else 0
        normalized_pr = (pagerank_dict.get(doc_id, 0) / max_pr) if max_pr else 0

        combined_score = (bm25_weight * normalized_bm25) + (pagerank_weight * normalized_pr)
        combined_scores[doc_id] = combined_score

    return combined_scores


nltk.download('punkt')
nltk.download('stopwords')
english_stopwords = frozenset(stopwords.words('english'))
corpus_stopwords = ["category", "references", "also", "external", "links",
                    "may", "first", "see", "history", "people", "one", "two",
                    "part", "thumb", "including", "second", "following",
                    "many", "however", "would", "became"]

all_stopwords = english_stopwords.union(corpus_stopwords)
porter = PorterStemmer()

def backend_search(query):
    tokens = nltk.word_tokenize(query)

    stemmed_tokens = [porter.stem(token) for token in tokens if token.isalpha() and token not in all_stopwords]

    bm25_scores = bm25_score(index_object, stemmed_tokens)
    sorted_scores = {k: v for k, v in sorted(bm25_scores.items(), key=lambda item: item[1], reverse=True)}

    # Using islice to slice the first 200 items
    bm25_scores_200 = dict(islice(sorted_scores.items(), 200))
    PageRank_Dict_200 = {k: PageRank_Dict[k] for k in bm25_scores_200 if k in PageRank_Dict}

    combined_scores = combine_bm25_pagerank(bm25_scores, PageRank_Dict_200)

    combined_scores_sorted = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)

    return [(str(doc_id), doc_id_to_title_dict[doc_id]) for doc_id, score in combined_scores_sorted[:30]]  # example: top 10 documents
